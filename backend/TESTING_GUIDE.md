# ðŸ§ª GuÃ­a de Testing - Tennis Analysis

## ðŸ“‹ Tabla de Contenidos
1. [InstalaciÃ³n](#instalaciÃ³n)
2. [EjecuciÃ³n de Tests](#ejecuciÃ³n-de-tests)
3. [Estructura de Tests](#estructura-de-tests)
4. [Cobertura de CÃ³digo](#cobertura-de-cÃ³digo)
5. [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

---

## ðŸš€ InstalaciÃ³n

### 1. Activar Entorno Virtual
```bash
# En WSL/Ubuntu
cd /mnt/c/users/hogar/tennis-analysis/backend
source venv/bin/activate
```

### 2. Instalar Dependencias de Testing
```bash
pip install -r requirements-testing.txt
```

---

## â–¶ï¸ EjecuciÃ³n de Tests

### Tests BÃ¡sicos
```bash
# Ejecutar todos los tests
pytest

# Ejecutar con verbosidad
pytest -v

# Ejecutar tests especÃ­ficos
pytest tests/test_extraer_historh2h.py

# Ejecutar una clase de tests especÃ­fica
pytest tests/test_extraer_historh2h.py::TestEloRatingSystem

# Ejecutar un test individual
pytest tests/test_extraer_historh2h.py::TestEloRatingSystem::test_initialization
```

### Tests por Marcadores
```bash
# Solo tests unitarios
pytest -m unit

# Solo tests de integraciÃ³n
pytest -m integration

# Excluir tests lentos
pytest -m "not slow"

# Tests de humo (smoke tests)
pytest -m smoke
```

### Tests con Cobertura
```bash
# Ejecutar con reporte de cobertura
pytest --cov=backend --cov-report=html

# Ver reporte en navegador
# El reporte HTML se genera en: htmlcov/index.html
```

### Tests en Modo Watch (Desarrollo)
```bash
# Instalar pytest-watch
pip install pytest-watch

# Ejecutar en modo watch
ptw
```

---

## ðŸ“ Estructura de Tests

```
tennis-analysis/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ extraer_historh2h.py
â”‚   â”œâ”€â”€ generar_dataset_plus.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                    # Fixtures globales
â”‚   â”œâ”€â”€ test_extraer_historh2h.py      # Tests para scraper H2H
â”‚   â”œâ”€â”€ test_generar_dataset_plus.py   # Tests para dataset
â”‚   â”œâ”€â”€ test_aplicar_enhancer.py       # Tests para enhancer
â”‚   â””â”€â”€ fixtures/                       # Datos de prueba
â”‚       â”œâ”€â”€ sample_matches.json
â”‚       â”œâ”€â”€ sample_rankings.json
â”‚       â””â”€â”€ ...
â”œâ”€â”€ pytest.ini                          # ConfiguraciÃ³n de pytest
â””â”€â”€ requirements-testing.txt            # Dependencias de testing
```

---

## ðŸ“Š Cobertura de CÃ³digo

### Objetivo de Cobertura
- **MÃ­nimo aceptable:** 70%
- **Objetivo ideal:** 85%
- **CrÃ­tico (clases core):** 90%+

### Ver Reporte de Cobertura
```bash
# Generar reporte
pytest --cov=backend --cov-report=term-missing

# Reporte HTML detallado
pytest --cov=backend --cov-report=html
open htmlcov/index.html  # En tu navegador
```

### Ejemplo de Salida
```
Name                           Stmts   Miss  Cover   Missing
------------------------------------------------------------
backend/extraer_historh2h.py     850     85    90%   234-245, 567-589
backend/generar_dataset_plus.py  420     63    85%   123-134, 456-478
------------------------------------------------------------
TOTAL                           1270    148    88%
```

---

## âœ… Mejores PrÃ¡cticas

### 1. Principios AAA (Arrange-Act-Assert)
```python
def test_elo_rating_update():
    # Arrange - Preparar datos
    elo = EloRatingSystem(k_factor=32)
    
    # Act - Ejecutar acciÃ³n
    elo.update_ratings("Winner", "Loser")
    
    # Assert - Verificar resultado
    assert elo.ratings["Winner"] > 1500
```

### 2. Usar Fixtures para Datos Reutilizables
```python
@pytest.fixture
def sample_player():
    return {
        'name': 'Rafael Nadal',
        'rank': 2,
        'points': 7000
    }

def test_player_ranking(sample_player):
    assert sample_player['rank'] == 2
```

### 3. Mocking de Dependencias Externas
```python
@patch('extraer_historh2h.requests.get')
def test_fetch_data(mock_get):
    mock_get.return_value.json.return_value = {'data': 'test'}
    result = fetch_player_data('test_id')
    assert result == {'data': 'test'}
```

### 4. Tests Parametrizados
```python
@pytest.mark.parametrize("rank,expected_elo", [
    (1, 2400),
    (10, 2200),
    (50, 2000),
    (100, 1800)
])
def test_elo_estimation(rank, expecte